{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f58d491b-6495-4f8a-81cf-ab6477b3dd9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    ">>> #### Big DAta Project: Analyzing Public Perception and Topics Surrounding Elon Musk on Twitter\n",
    "\n",
    "> <strong> Introduction\n",
    ">>\n",
    "      In today's digital age, social media platforms serve as rich sources of data for understanding public sentiment and opinion on various topics\n",
    "      This project focuses on analyzing Twitter data related to Elon Musk, a prominent figure in the tech industry, between November 21 to 22, 2022\n",
    "      The objective is to gain insights into public sentiment towards Elon Musk during this period and extract meaningful topics from the tweets\n",
    "\n",
    "> <strong> Objectives\n",
    ">>\n",
    "      . Collect Twitter data containing mentions of Elon Musk\n",
    "      . Clean and preprocess the collected data\n",
    "      . Analyze sentiment and derive topics from the tweets\n",
    "      . Train and evaluate machine learning models to predict sentiment\n",
    "      . Create a dashboard to visualize insights from the data\n",
    "\n",
    "> <strong> Methodology\n",
    ">>\n",
    "      - Data Collection: twitter API is integrated with AWS Data Firehose and S3 bucket via EC2 instance\n",
    "      - Data Cleaning: the dataset is securely transfered from S3 bucket to MS Azure Databricks for wrangling \n",
    "      - Sentiment Analysis: for the NLP, pretrained model from Hugging Face Transformer library is employed\n",
    "      - Topic Modeling: applied topic modeling algorithms to identify prevalent themes and discussions\n",
    "      - Feature transformation and Model Training: transformed the tweet to chunks suitable for training and validating ML algorithm\n",
    "      - Pipeline: pipelines are created for the trained models, and the resulting datasets were exported to a private S3 bucket for further analysis \n",
    "      - Visualization and Interpretation: tables are generated from the stored dataset using AWS Athena. The curated data is then visualized QuickSight dashboard\n",
    "\n",
    "> <strong> Outcomes\n",
    ">>\n",
    "      . Identification of dominant sentiment clusters and key themes emerging from the Twitter data\n",
    "      . Insights into public sentiment towards Elon Musk were gained, revealing a majority of negative sentiments during the specified time period\n",
    "      . Visualization of sentiment trends and topic distributions to facilitate understanding and interpretation of the data\n",
    "\n",
    "> <strong> Conclusions\n",
    ">>    \n",
    "      This project successfully demonstrated the process of collecting, processing, and analyzing Twitter data related to Elon Musk\n",
    "      By leveraging ML & visualization tools, valuable insights are obtained, highlighting the importance of understanding public sentiment in the digital age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68075cfe-e663-46bf-bc62-ca21bc6716b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.  INITIALIZE SPARK SESSION AND INSTALL LIBRARIES\n",
    "> <strong> steps\n",
    ">>\n",
    "      - install libraries to support Hugging Face Transformer\n",
    "      - initialize spark session and spark contents\n",
    "      - import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "41a4b6a4-ba4f-4a63-b9ea-fb7fb9004299",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# install required libraries\n",
    "\n",
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install --upgrade numpy\n",
    "%pip install bertopic\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "89d60ff8-c5c4-48bf-bc8e-fdd0d5ca6c8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# initialize spark session and contents\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('BigDataProject') \\\n",
    "        .getOrCreate()\n",
    "print('Session created')\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "317407f5-706e-4ea2-9a31-1e5759af2afe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF, Tokenizer, NGram, ChiSqSelector, VectorAssembler, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f87315c9-468a-4bb2-be20-f7fa1cb662a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.  MOUNT AND LOAD S3 BUCKET DIRECTORY\n",
    "> <strong> steps and purpose\n",
    ">>\n",
    "      - define function for mounting s3 bucket which will be use as reference to s3 directories\n",
    "      - mount the s3 directory where the scrapped data from firehose is stored\n",
    "      - mount my s3 buckets which are used to store and retrieve data for cleaning, preprocessing, and analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "54d55965-1d23-42cb-86f6-7adc8a4311cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function to mount s3 buckets\n",
    "\n",
    "def mount_s3_bucket(access_key, secret_key, bucket_name, mount_folder):\n",
    "  ACCESS_KEY_ID = access_key\n",
    "  SECRET_ACCESS_KEY = secret_key\n",
    "  ENCODED_SECRET_KEY = SECRET_ACCESS_KEY.replace(\"/\", \"%2F\")\n",
    "  print(\"Mounting\", bucket_name)\n",
    "  try:\n",
    "    # unmount the data in case it was already mounted.\n",
    "    dbutils.fs.unmount(\"/mnt/%s\" % mount_folder)\n",
    "  except:\n",
    "    # if it fails to unmount it most likely wasn't mounted in the first place\n",
    "    print (\"Directory not unmounted: \", mount_folder)\n",
    "  finally:\n",
    "    # lastly, mount the bucket.\n",
    "    dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY_ID, ENCODED_SECRET_KEY, bucket_name), \"/mnt/%s\" % mount_folder)\n",
    "    #dbutils.fs.mount(\"s3a://\"+ ACCESS_KEY_ID + \":\" + ENCODED_SECRET_KEY + \"@\" + bucket_name, mount_folder)\n",
    "    print(\"The bucket\", bucket_name, \"was mounted to\", mount_folder, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3b9d28d6-1045-4e30-9094-f465df74b429",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set AWS programmatic access credentials\n",
    "\n",
    "ACCESS_KEY = dbutils.secrets.get(scope=\"aws_key\", key=\"AccesskeyID\")\n",
    "SECRET_ACCESS_KEY = dbutils.secrets.get(scope=\"aws_key\", key=\"Secretaccesskey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d30c72b4-9a3f-4dbf-b958-a7dfc3067f8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mount Amazon Data Firehose S3 Bbucket directory\n",
    "\n",
    "mount_s3_bucket(ACCESS_KEY, SECRET_ACCESS_KEY, 'weclouddata/twitter', 'wcd_twt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1715e9b6-4f30-4472-be0b-e0741758a7a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mount my own AWS s3 bucket directories\n",
    "\n",
    "mount_s3_bucket(ACCESS_KEY, SECRET_ACCESS_KEY, 'bigdatapro/Project/raw_dataset/', 'raw_data')\n",
    "mount_s3_bucket(ACCESS_KEY, SECRET_ACCESS_KEY, 'bigdatapro/Project/clean_dataset/', 'clean_data')\n",
    "mount_s3_bucket(ACCESS_KEY, SECRET_ACCESS_KEY, 'bigdatapro/Project/prediction_dataset/', 'pred_data')\n",
    "mount_s3_bucket(ACCESS_KEY, SECRET_ACCESS_KEY, 'bigdatapro/Project/plot_output/', 'figures')\n",
    "mount_s3_bucket(ACCESS_KEY, SECRET_ACCESS_KEY, 'bigdatapro/Project/topic_dataset1/', 'topic_data1')\n",
    "mount_s3_bucket(ACCESS_KEY, SECRET_ACCESS_KEY, 'bigdatapro/Project/topic_dataset2/', 'topic_data2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "74946b51-01f2-4815-9a50-5e4b995b987b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /mnt/wcd_twt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0b9247d8-f775-4d31-91b6-008a9e14be15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /mnt/wcd_twt/ElonMusk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50a5d2c1-9eea-4068-9002-961a0e428eec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define dataset schema for scrapped Elon Musk tweets\n",
    "\n",
    "elonmusk_schema = StructType([\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('username', StringType(), True),\n",
    "    StructField('tweet', StringType(), True),\n",
    "    StructField('followers', IntegerType(), True),\n",
    "    StructField('location', StringType(), True),\n",
    "    StructField('geo', StringType(), True),\n",
    "    StructField('when', StringType(), True),\n",
    "    StructField('others', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac7e930-99f5-4907-869c-fde55149ea3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read the dataset with given schema\n",
    "\n",
    "wcd_file_path = '/mnt/wcd_twt/ElonMusk/*/*/*/*/*'\n",
    "elonmusk_raw = (spark.read\n",
    "       .option(\"header\", \"false\")\n",
    "       .option(\"delimiter\", \"\\t\")\n",
    "       .schema(elonmusk_schema)\n",
    "       .csv(wcd_file_path))\n",
    "display(elonmusk_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bffae318-9fd1-42ab-b19a-0c71c15c845c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /mnt/raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "48136b3e-f525-402d-b01b-70b6bb2c54f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write the raw constructed df above to my s3 bucket\n",
    "\n",
    "raw_filepath = \"/mnt/raw_data\"\n",
    "(elonmusk_raw.write\n",
    "  .option(\"delimiter\", \"\\t\")  \n",
    "  .option(\"header\", \"false\")\n",
    "  .mode(\"overwrite\")\n",
    "  .csv(raw_filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "df5ff20b-68ab-4e94-a823-00e323e488ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /mnt/raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53e9d516-eaff-4a5b-b6f5-74b7283a0f94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3. CLEAN THE DATASET\n",
    "> <strong> cleaning steps\n",
    ">>\n",
    "        - select all columns needed for sentiment analysis\n",
    "        - convert date_time column to timestamp\n",
    "        - remove URLs from the 'tweet' column and create a new column 'tweet' with the cleaned text\n",
    "        - replace any non-alphabetic characters in the 'tweet' column with a space and create a new column 'tweet' with the cleaned text\n",
    "        - replace consecutive spaces with a single space in the 'tweet' column and create a new column 'tweet' with the cleaned text\n",
    "        - convert all characters in the 'tweet' column to lowercase and create a new column 'tweet' with the cleaned text\n",
    "        - trim leading and trailing spaces from the 'tweet' column and create a new column 'tweet' with the cleaned text\n",
    "        - remove the rows where the tweet column is NaN or NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "03b52e6a-6d31-4a68-8c15-9fb12b6c95d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# clean and cache the dataset\n",
    "spark.sql('set spark.sql.legacy.timeParserPolicy=LEGACY')\n",
    "\n",
    "# select columns for sentiment analysis\n",
    "elonmusk_senti = elonmusk_raw.select('tweet', 'followers', 'location', F.col('when').alias('date_time'))\n",
    "\n",
    "# clean the 'tweet' and date_time columns\n",
    "elonmusk_clean = elonmusk_senti.withColumn('date_time', F.to_timestamp(F.col('date_time'), 'EEE MMM dd HH:mm:ss Z yyyy')) \\\n",
    "    .withColumn('tweet', F.regexp_replace('tweet', r\"http\\S+\", \"\")) \\\n",
    "    .withColumn('tweet', F.regexp_replace('tweet', r\"[^a-zA-z]\", \" \")) \\\n",
    "    .withColumn('tweet', F.regexp_replace('tweet', r\"\\s+\", \" \")) \\\n",
    "    .withColumn('tweet', F.lower('tweet')) \\\n",
    "    .withColumn('tweet', F.trim('tweet')) \\\n",
    "    .na.drop(subset=['tweet'])\n",
    "display(elonmusk_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62976426-56ed-4769-a91c-d0a07d7a5d7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4. CREATE THE SENTIMENT COLUMN USING PRETRAINED MODEL\n",
    "\n",
    "> <strong> steps to create sentiment column for each tweet using pretrained model from hugging face transformer\n",
    "\n",
    ">>\n",
    "      - import all necessary libraries\n",
    "      - initialize the model for sentiment analysis\n",
    "      - use user define function to apply the model to 'tweet' column and create a  new column for the sentiment\n",
    "      - replace the sentiments with numerical equivalent\n",
    "      - drop unused column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a1581e28-45b0-4cd2-9e6d-242c19d4043b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# generate sentiment/label column\n",
    "\n",
    "# load the RoBERTa classifier model from hugging face transformer\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "roberta_classifier = pipeline(task='sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment', max_length=512, device=device, truncation=True)\n",
    "\n",
    "# define a UDF to apply the RoBERTa classifier for sentiment analysis\n",
    "def predict_sentiment(tweet):\n",
    "    result = roberta_classifier.predict(tweet)[0]\n",
    "    return (result['label'], float(result['score']))\n",
    "\n",
    "# apply the UDF to create a new column 'roberta_sentiment'\n",
    "schema = StructType([StructField('label', StringType(), True), StructField('score', FloatType(), True)])\n",
    "predict_sentiment_udf = F.udf(predict_sentiment, schema)\n",
    "elonmusk_clean = elonmusk_clean.withColumn('roberta_sentiment', predict_sentiment_udf(F.col('tweet')))\n",
    "\n",
    "# extract the label from the struct and create new column called roberta_label\n",
    "elonmusk_clean = elonmusk_clean.withColumn('roberta_label', F.col('roberta_sentiment').getField('label'))\n",
    "\n",
    "# replace the label values with numerical equivalents (Positive/LABEL_2: 2, Neutral/LABEL_1: 1, Negative/LABEL_0: 0)\n",
    "elonmusk_clean = elonmusk_clean.withColumn('roberta_label', \n",
    "                                         F.when(F.col('roberta_label') == 'LABEL_2', 2)\n",
    "                                         .when(F.col('roberta_label') == 'LABEL_0', 0)\n",
    "                                         .when(F.col('roberta_label') == 'LABEL_1', 1)\n",
    "                                         .otherwise(None))\n",
    "\n",
    "# drop roberta_sentiment column\n",
    "elonmusk_clean = elonmusk_clean.drop('roberta_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5a512a-bd20-4046-bd94-994aa5754e9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# list all existing columns\n",
    "print(elonmusk_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "93c8a52d-756a-4576-a0e6-4fa640fb8f8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(elonmusk_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0893721-ba1c-4b4b-a235-3bc180aeca29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5. USE S3 BUCKET AS CACHE STORAGE\n",
    "> <strong> required steps\n",
    ">>\n",
    "      - write the cleaned sentiment table into my aws s3 bucket\n",
    "      - define the schema to read back the dataset from s3 bucket\n",
    "      - read the dataset from s3 bucket for easy recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5d043416-2b40-484d-a200-00b43203bde4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write the clean dataset to my s3 bucket for easy reference\n",
    "\n",
    "clean_filepath = \"/mnt/clean_data\"\n",
    "(elonmusk_clean.write\n",
    "  .option(\"delimiter\", \"\\t\")  \n",
    "  .option(\"header\", \"false\")\n",
    "  .mode(\"overwrite\")\n",
    "  .csv(clean_filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "13480ba4-5335-413b-a3d3-f4460d136099",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read the clean dataset from my s3 bucket for faster compute\n",
    "re_schema = StructType([\n",
    "    StructField('tweet', StringType(), True),\n",
    "    StructField('followers', IntegerType(), True),\n",
    "    StructField('location', StringType(), True),\n",
    "    StructField('date_time', TimestampType(), True),\n",
    "    StructField('label', IntegerType(), True)])\n",
    "\n",
    "re_elonmusk = (spark.read\n",
    "       .option(\"header\", \"false\")\n",
    "       .option(\"delimiter\", \"\\t\")\n",
    "       .schema(re_schema)\n",
    "       .csv(\"/mnt/clean_data\"))\n",
    "re_elonmusk.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e299a6cf-bc46-4b3c-81ce-6bacce9dc721",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 6. TRANSFORM FEATURES\n",
    "> <strong> transformation steps include\n",
    ">>\n",
    "      - tokenization\n",
    "      - stopword removal\n",
    "      - count vectorization\n",
    "      - term frequency - inverse document frequency vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "37ea2f31-2192-4f3a-802d-f7d201e86fa2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# break down the strings in tweet to individual words/tokens\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"tokens\")\n",
    "tweets_tokenized = tokenizer.transform(re_elonmusk)\n",
    "tweets_tokenized.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d14baa1a-a7b3-47d2-a9ee-d495459dde22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filter out stopwords\n",
    "\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "tweets_stopword = stopword_remover.transform(tweets_tokenized)\n",
    "tweets_stopword.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "09c006d2-f6ca-45f3-b9bc-c8e6f2e14a62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create index for individual word and then include the number of times they occur\n",
    "\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "cv_model = cv.fit(tweets_stopword)\n",
    "tweets_cv = cv_model.transform(tweets_stopword)\n",
    "tweets_cv.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f16107c5-39b3-4ae7-b79d-d11e0ba32384",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# penalizing words appearing more frequently and assigning more weight (rewards) to less frequent word\n",
    "\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5)\n",
    "idf_model = idf.fit(tweets_cv)\n",
    "tweets_idf = idf_model.transform(tweets_cv)\n",
    "tweets_idf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4330881-1579-4179-ab1a-420e87cbaaf6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 7. TRAIN AND EVALUATE MODEL\n",
    "> <strong> steps required\n",
    ">>\n",
    "      - perform train test split using 80% and 20% respectively\n",
    "      - train and test using two different ML algorithms\n",
    "      - evaluate the performance of the model using accuracy and roc-auc score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a4b8d8a6-5d03-4efb-8dd6-9cc15891251d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# perform train and test splitting\n",
    "\n",
    "train, test = tweets_idf.randomSplit([0.8, 0.2], seed=20240531)\n",
    "print(f'train_count: {train.count()}, test_count: {test.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f0bd7b33-082b-4f51-958a-3f89907e273c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model_1: Logistic Regression Classifier\n",
    "\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "lr_model = lr.fit(train)\n",
    "lr_pred = lr_model.transform(test)\n",
    "display(lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "23ad9b02-6952-4103-80da-bbc505412302",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model_1 Evaluation\n",
    "\n",
    "lr_eval = MulticlassClassificationEvaluator()\n",
    "lr_roc_auc = lr_eval.evaluate(lr_pred)\n",
    "lr_accuracy = lr_pred.filter(lr_pred.label == lr_pred.prediction).count() / float(lr_pred.count())\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(lr_accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(lr_roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "07254ca0-353d-4f95-adaf-7c5ef44a5d15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model_2: NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(modelType='multinomial')\n",
    "nb_model = nb.fit(train)\n",
    "nb_pred = nb_model.transform(test)\n",
    "nb_pred.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0c543909-61b7-469f-a81d-99f1d9ff2a5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model_2 Evaluation\n",
    "\n",
    "nb_eval = MulticlassClassificationEvaluator()\n",
    "nb_roc_auc = nb_eval.evaluate(nb_pred)\n",
    "nb_accuracy = nb_pred.filter(nb_pred.label == nb_pred.prediction).count() / float(nb_pred.count())\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(nb_accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(nb_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "529743ad-a2ce-4784-b5d3-c06299a67179",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 8. CREATE PIPELINE AND SAVE PREDICTIONS\n",
    "> <STRONG> steps include\n",
    ">> \n",
    "      - add all the cleaning and preprocessing steps to a pipeline\n",
    "      - include n-gram feature transformation in the pipeline\n",
    "      - base on best ML algorithm, train and predict the sentiment label using the pipeline\n",
    "      - run the pipeline and save the prediction into s3 bucket for analysis and dashboard\n",
    "      - create pipeline for model 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4821179d-2557-46dd-812d-39dd15ba9a78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# best model pipeline (model1)\n",
    "\n",
    "# perform train test split\n",
    "X_train, y_test = re_elonmusk.randomSplit([0.8, 0.2], seed=20240531)\n",
    "\n",
    "# create transformers for the ML pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"tokens\")\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"1gram_idf\", minDocFreq=5)\n",
    "ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"2gram\")\n",
    "ngram_hashingtf = HashingTF(inputCol=\"2gram\", outputCol=\"2gram_tf\", numFeatures=20000)\n",
    "ngram_idf = IDF(inputCol='2gram_tf', outputCol=\"2gram_idf\", minDocFreq=5) \n",
    "\n",
    "# assemble all text features\n",
    "assembler = VectorAssembler(inputCols=[\"1gram_idf\", \"2gram_tf\"], outputCol=\"rawFeatures\")\n",
    "\n",
    "# Chi-square variable selection\n",
    "selector = ChiSqSelector(numTopFeatures=2**14,featuresCol='rawFeatures', outputCol=\"features\")\n",
    "\n",
    "# regression model estimator\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "\n",
    "# build the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stopword_remover, cv, idf, ngram, ngram_hashingtf, ngram_idf, assembler, selector, lr])\n",
    "\n",
    "# pipeline model fitting\n",
    "pipeline_model = pipeline.fit(X_train)\n",
    "y_pred = pipeline_model.transform(y_test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "accuracy = y_pred.filter(y_pred.label == y_pred.prediction).count() / float(y_test.count())\n",
    "roc_auc = evaluator.evaluate(y_pred)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e2bb12f1-0de9-4bdb-b366-938d87369259",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# select prediction column and other columns needed analysis and dashboard\n",
    "save_prediction = lr_pred.select('tweet', 'followers', 'location', 'date_time', 'label', 'prediction')\n",
    "\n",
    "# save the prediction table as .csv file into s3 bucket\n",
    "pred_filepath = \"/mnt/clean_data\"\n",
    "(save_prediction.write\n",
    "  .option(\"delimiter\", \"\\t\")  \n",
    "  .option(\"header\", \"false\")\n",
    "  .mode(\"overwrite\")\n",
    "  .csv(pred_filepath))\n",
    "\n",
    "# # to read the predicted dataset from my s3 bucket, use this schema\n",
    "# pred_schema = StructType([\n",
    "#     StructField('tweet', StringType(), True),\n",
    "#     StructField('followers', IntegerType(), True),\n",
    "#     StructField('location', StringType(), True),\n",
    "#     StructField('date_time', TimestampType(), True),\n",
    "#     StructField('label', IntegerType(), True),\n",
    "#     StructField('prediction', DoubleType(), True)])\n",
    "\n",
    "# readme = (spark.read\n",
    "#        .option(\"header\", \"false\")\n",
    "#        .option(\"delimiter\", \"\\t\")\n",
    "#        .schema(pred_schema)\n",
    "#        .csv(\"/mnt/pred_data\"))\n",
    "# readme.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cd5abacf-8ad2-44bd-adb9-2852b0eb8221",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pipeline for model 2\n",
    "\n",
    "# perform train test split\n",
    "X_train, y_test = re_elonmusk.randomSplit([0.8, 0.2], seed=20240531)\n",
    "\n",
    "# create transformers for the ML pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"tokens\")\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"1gram_idf\", minDocFreq=5)\n",
    "ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"2gram\")\n",
    "ngram_hashingtf = HashingTF(inputCol=\"2gram\", outputCol=\"2gram_tf\", numFeatures=20000)\n",
    "ngram_idf = IDF(inputCol='2gram_tf', outputCol=\"2gram_idf\", minDocFreq=5) \n",
    "\n",
    "# assemble all text features\n",
    "assembler = VectorAssembler(inputCols=[\"1gram_idf\", \"2gram_tf\"], outputCol=\"rawFeatures\")\n",
    "\n",
    "# Chi-square variable selection\n",
    "selector = ChiSqSelector(numTopFeatures=2**14,featuresCol='rawFeatures', outputCol=\"features\")\n",
    "\n",
    "# NaiveBayes model estimator\n",
    "nb = NaiveBayes(modelType='multinomial')\n",
    "\n",
    "# build the pipeline\n",
    "pipeline_1 = Pipeline(stages=[tokenizer, stopword_remover, cv, idf, ngram, ngram_hashingtf, ngram_idf, assembler, selector, nb])\n",
    "\n",
    "# pipeline model fitting\n",
    "nb_pipeline = pipeline_1.fit(X_train)\n",
    "y_pred = nb_pipeline.transform(y_test)\n",
    "\n",
    "evaluator_1 = MulticlassClassificationEvaluator()\n",
    "nb_accuracy = y_pred.filter(y_pred.label == y_pred.prediction).count() / float(y_test.count())\n",
    "nb_roc_auc = evaluator_1.evaluate(y_pred)\n",
    "\n",
    "print(\"NB Accuracy Score: {0:.4f}\".format(nb_accuracy))\n",
    "print(\"NB ROC-AUC: {0:.4f}\".format(nb_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f66fa93d-66a0-4b2d-8203-3075a2835cbc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 9. TOPIC MODELING WITH HUGGING FACE\n",
    "> <STRONG> steps include\n",
    ">> \n",
    "      - load required libraries\n",
    "      - extract the tweet column and convert it to a list of text\n",
    "      - remove stopwords\n",
    "      - initialize, train and transform BERTopic model\n",
    "      - perform analysis\n",
    "      - save the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "026f8b19-09d0-4cab-bf59-b244e78cde09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP_1:\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# select the needed columns for topic analysis\n",
    "topic_data_df = re_elonmusk.select(\"tweet\", \"label\", \"followers\")\n",
    "\n",
    "# tokenize the 'tweet' column\n",
    "tweet_token = Tokenizer(inputCol=\"tweet\", outputCol=\"tokenized_tweet\")\n",
    "topic_data_df = tweet_token.transform(topic_data_df)\n",
    "\n",
    "# remove stopwords from the 'tokenized_tweet' column\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokenized_tweet\", outputCol=\"clean_tweet\")\n",
    "topic_data_df = stopword_remover.transform(topic_data_df)\n",
    "\n",
    "# extract the 'clean_tweet' column as a list of documents\n",
    "documents = topic_data_df.select(\"clean_tweet\").rdd.flatMap(lambda row: [' '.join(row.clean_tweet)]).collect()\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "030e7d71-6dc4-4fd0-acdd-408ec226aa9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP_2\n",
    "## train and transform the model\n",
    "\n",
    "# initialize BERTopic model\n",
    "model = BERTopic(nr_topics=21, verbose=True)\n",
    "\n",
    "# fit and transform the model on the list\n",
    "topics, _ = model.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3173bb5c-74f2-4e31-9086-51478b173eae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP_3a\n",
    "## assign topic to each tweet/row\n",
    "\n",
    "# convert the list of topics to a Spark DataFrame with a single column\n",
    "topics_df = spark.createDataFrame([(index, topic) for index, topic in enumerate(topics)], [\"row_index\", \"topic\"])\n",
    "\n",
    "# add a row index to the original DataFrame for joining\n",
    "topic_data_df = topic_data_df.withColumn(\"row_index\", F.monotonically_increasing_id())\n",
    "\n",
    "# assign the topics to the original DataFrame\n",
    "topic_data_df = topic_data_df.join(topics_df, on=\"row_index\", how=\"inner\").drop(\"row_index\")\n",
    "topic_data_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f18add15-dd21-457c-b65e-962bd356be5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP_3b\n",
    "## save the updated topic dataframe to my s3 bucket\n",
    "\n",
    "# zip the topic_data_df with the list of documents created in step 1\n",
    "topic_data_with_documents = topic_data_df.rdd.zipWithIndex().map(lambda x: Row(**dict(x[0].asDict(), documents=documents[x[1]]))).toDF()\n",
    "\n",
    "# drop some columns before saving\n",
    "topic_data_with_documents = topic_data_with_documents.drop(\"tokenized_tweet\", \"clean_tweet\")\n",
    "\n",
    "# save the updated dataframe as parquet file into s3 bucket\n",
    "topic_filepath1 = \"/mnt/topic_data1\"\n",
    "(topic_data_with_documents.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option('compression', 'snappy')\n",
    "    .parquet(topic_filepath1))\n",
    "topic_data_with_documents.limit(3).show(truncate=True)\n",
    "\n",
    "# # read the Parquet file into a DataFrame\n",
    "# reading_parquet = spark.read.parquet(\"/mnt/topic_data1\")\n",
    "# display(reading_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "35d27340-c09e-40fc-bd7e-392b45a2d9d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP_4\n",
    "\n",
    "# display the topic frequent identified in the dataset\n",
    "## -1 due to outliers including: empty docs, low confidence assignment, duplicates docs among others\n",
    "most_frequent_topics = model.get_topic_freq()\n",
    "display(most_frequent_topics)\n",
    "print(\"Total number of topics:\", model.get_topic_freq().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3099deec-cb48-4424-a9f3-9b97f333daf4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP_5\n",
    "\n",
    "# extract the top 10 words for each topic\n",
    "top_words_lists = []\n",
    "for topic_index in range(-1,20):\n",
    "    top_words = model.get_topic(topic_index)\n",
    "    top_words = [item[0] for item in top_words]\n",
    "    top_words_lists.append((topic_index, top_words))\n",
    "\n",
    "# create a list of Row objects to represent each topic with its top words\n",
    "unique_topic_rows = [Row(topic_index=topic[0], top_words=topic[1]) for topic in top_words_lists]\n",
    "\n",
    "# Create a DataFrame from the list of Row objects with the specified schema\n",
    "unique_topic_df = spark.createDataFrame(unique_topic_rows, schema=[\"topic_index\", \"top_words\"])\n",
    "\n",
    "# save the unique topic dataframe as parquet file into s3 bucket\n",
    "topic_filepath2 = \"/mnt/topic_data2\"\n",
    "(unique_topic_df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option('compression', 'snappy')\n",
    "    .parquet(topic_filepath2))\n",
    "unique_topic_df.limit(3).show(truncate=False)\n",
    "\n",
    "# read the Parquet file into a DataFrame\n",
    "# reading_parquet = spark.read.parquet(\"/mnt/topic_data2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "065beb73-a5d3-45b7-bf8c-38399c5a205a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP_6a\n",
    "\n",
    "# plot the Intertopic Distance Map to understand topic\n",
    "model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "22dd5521-cbdf-4a88-a7d0-0297e14edbcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP_6b\n",
    "\n",
    "#  visualize topics with c-TF-IDF scores and save to s3 bucket\n",
    "model.visualize_barchart()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "759e672c-d04c-44c6-9b1b-7f5815089ce3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP_6c\n",
    "\n",
    "# visualize topic similarity with the heatmap\n",
    "model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b918dc5a-7c96-448b-9d94-d394d850d7d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP_7\n",
    "\n",
    "# save the model for reuse\n",
    "model.save(\"new_model\")\n",
    "model = BERTopic.load(\"new_model\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2134804640007457,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BigDataProject 20240520",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
